{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1ILOiTeDsC3UshY0xMI91CTxEjlbcg2fd",
      "authorship_tag": "ABX9TyMOKdn49pHP4OdgVZc6swpN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shcho999/block-explorer-tutorials/blob/main/00_reinforcement_learning_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html"
      ],
      "metadata": {
        "id": "1prRcLa0Onoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env"
      ],
      "metadata": {
        "id": "gCfwv2AnUQ0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeAgRUTFOiYr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/reinforcement')\n",
        "from gridworld import GridWorld # Changed to absolute import\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uA0JOlOJX8Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_utility(utility_matrix, observation, new_observation,\n",
        "                   reward, alpha, gamma):\n",
        "    '''Return the updated utility matrix\n",
        "\n",
        "    @param utility_matrix the matrix before the update\n",
        "    @param observation the state observed at t\n",
        "    @param new_observation the state observed at t+1\n",
        "    @param reward the reward observed after the action\n",
        "    @param alpha the step size (learning rate)\n",
        "    @param gamma the discount factor\n",
        "    @return the updated utility matrix\n",
        "    '''\n",
        "    u = utility_matrix[observation[0], observation[1]]\n",
        "    u_t1 = utility_matrix[new_observation[0], new_observation[1]]\n",
        "    utility_matrix[observation[0], observation[1]] += \\\n",
        "        alpha * (reward + gamma * u_t1 - u)\n",
        "    return utility_matrix"
      ],
      "metadata": {
        "id": "M3GfjzjIciPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action"
      ],
      "metadata": {
        "id": "HghPkBONB9Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 주어진 ploicy matrix기준으로 출발점을 고정하고 policy 기준 action을 진행하면서 발생하는 reward를 이용 Temporal Differencing 기준으로 utility matrix를 업데이트한다."
      ],
      "metadata": {
        "id": "kZ7L5j5zSOrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    env = GridWorld(3, 4)\n",
        "\n",
        "    #Define the state matrix\n",
        "    state_matrix = np.zeros((3,4))\n",
        "    state_matrix[0, 3] = 1\n",
        "    state_matrix[1, 3] = 1\n",
        "    state_matrix[1, 1] = -1\n",
        "    print(\"State Matrix:\")\n",
        "    print(state_matrix)\n",
        "\n",
        "    #Define the reward matrix\n",
        "    reward_matrix = np.full((3,4), -0.04)\n",
        "    reward_matrix[0, 3] = 1\n",
        "    reward_matrix[1, 3] = -1\n",
        "    print(\"Reward Matrix:\")\n",
        "    print(reward_matrix)\n",
        "\n",
        "    #Define the transition matrix\n",
        "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
        "                                  [0.1, 0.8, 0.1, 0.0],\n",
        "                                  [0.0, 0.1, 0.8, 0.1],\n",
        "                                  [0.1, 0.0, 0.1, 0.8]])\n",
        "\n",
        "    #Define the policy matrix\n",
        "    #This is the optimal policy for world with reward=-0.04\n",
        "    policy_matrix = np.array([[1,      1,  1,  -1],\n",
        "                              [0, np.nan,  0,  -1],\n",
        "                              [0,      3,  3,   3]])\n",
        "    print(\"Policy Matrix:\")\n",
        "    print(policy_matrix)\n",
        "\n",
        "    env.setStateMatrix(state_matrix)\n",
        "    env.setRewardMatrix(reward_matrix)\n",
        "    env.setTransitionMatrix(transition_matrix)\n",
        "\n",
        "    utility_matrix = np.zeros((3,4))\n",
        "    gamma = 0.999\n",
        "    alpha = 0.1 #constant step size\n",
        "    tot_epoch = 300000\n",
        "    print_epoch = 1000\n",
        "\n",
        "    for epoch in range(tot_epoch):\n",
        "        #Reset and return the first observation\n",
        "        observation = env.reset(exploring_starts=False)\n",
        "        for step in range(1000):\n",
        "            #Take the action from the action matrix\n",
        "            action = policy_matrix[observation[0], observation[1]]\n",
        "            #Move one step in the environment and get obs and reward\n",
        "            new_observation, reward, done = env.step(action)\n",
        "            utility_matrix = update_utility(utility_matrix, observation,\n",
        "                                            new_observation, reward, alpha, gamma)\n",
        "            observation = new_observation\n",
        "            #print(utility_matrix)\n",
        "            if done: break\n",
        "\n",
        "        if(epoch % print_epoch == 0):\n",
        "            print(\"\")\n",
        "            print(\"Utility matrix after \" + str(epoch+1) + \" iterations:\")\n",
        "            print(utility_matrix)\n",
        "    #Time to check the utility matrix obtained\n",
        "    print(\"Utility matrix after \" + str(tot_epoch) + \" iterations:\")\n",
        "    print(utility_matrix)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mHBM8iy8X9JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### update with trace matrix"
      ],
      "metadata": {
        "id": "wJMc-dMZXvim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### patr 2의 MCMC방법이 하나의 에피소드가 완결된 이후에 에피소드의 각각의 step에서 발생하는 reward를 각 스템기준으로 현재가치화하여 각 스테이스의 가치를 결정(action 기준도 동일)\n",
        "\n",
        "##### 이와 다르게 현재의 방법론은 에피스드가 진행되는 각각 스텝에서 reward를 계산하고 이것을 과거의 지난온 step의 value계산을 업데이트한다. 업데이트 하는 방법은 현재 스템과 지난 스텝간의 거리준으로 할인하여 현재 reward를 과거 스템가치 계산에 업데이트한다.\n",
        "\n",
        "##### 현재의 방법은 업데트를 MCMC방법에 비해 신속하게 하면서 하나의 에피스드에 속한 각각 스텝이 에피소드 최종결과에 영향을 받아 업데이트 되는 결과를 가져온다. 따라서 value관점에서 업데이트가 빠르게 진행되고 결과적으로 수렴속도가 빠르게 진행된다."
      ],
      "metadata": {
        "id": "0ByxDPxmZEqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_utility(utility_matrix, trace_matrix, alpha, delta):\n",
        "    '''Return the updated utility matrix\n",
        "\n",
        "    @param utility_matrix the matrix before the update\n",
        "    @param alpha the step size (learning rate)\n",
        "    @param delta the error (Taget-OldEstimte)\n",
        "    @return the updated utility matrix\n",
        "    '''\n",
        "    utility_matrix += alpha * delta * trace_matrix\n",
        "    return utility_matrix\n",
        "\n",
        "def update_eligibility(trace_matrix, gamma, lambda_):\n",
        "    '''Return the updated trace_matrix\n",
        "\n",
        "    @param trace_matrix the eligibility traces matrix\n",
        "    @param gamma discount factor\n",
        "    @param lambda_ the decaying value\n",
        "    @return the updated trace_matrix\n",
        "    '''\n",
        "    trace_matrix = trace_matrix * gamma * lambda_\n",
        "    return trace_matrix\n"
      ],
      "metadata": {
        "id": "sNd8srbicWj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oHJhOiyunApU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    env = GridWorld(3, 4)\n",
        "\n",
        "    #Define the state matrix\n",
        "    state_matrix = np.zeros((3,4))\n",
        "    state_matrix[0, 3] = 1\n",
        "    state_matrix[1, 3] = 1\n",
        "    state_matrix[1, 1] = -1\n",
        "    print(\"State Matrix:\")\n",
        "    print(state_matrix)\n",
        "\n",
        "    #Define the reward matrix\n",
        "    reward_matrix = np.full((3,4), -0.04)\n",
        "    reward_matrix[0, 3] = 1\n",
        "    reward_matrix[1, 3] = -1\n",
        "    print(\"Reward Matrix:\")\n",
        "    print(reward_matrix)\n",
        "\n",
        "    #Define the transition matrix\n",
        "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
        "                                  [0.1, 0.8, 0.1, 0.0],\n",
        "                                  [0.0, 0.1, 0.8, 0.1],\n",
        "                                  [0.1, 0.0, 0.1, 0.8]])\n",
        "\n",
        "    #Define the policy matrix\n",
        "    #This is the optimal policy for world with reward=-0.04\n",
        "    policy_matrix = np.array([[1,      1,  1,  -1],\n",
        "                              [0, np.nan,  0,  -1],\n",
        "                              [0,      3,  3,   3]])\n",
        "    print(\"Policy Matrix:\")\n",
        "    print(policy_matrix)\n",
        "\n",
        "    #Define and print the eligibility trace matrix\n",
        "    trace_matrix = np.zeros((3,4))\n",
        "    print(\"Trace Matrix:\")\n",
        "    print(trace_matrix)\n",
        "\n",
        "    env.setStateMatrix(state_matrix)\n",
        "    env.setRewardMatrix(reward_matrix)\n",
        "    env.setTransitionMatrix(transition_matrix)\n",
        "\n",
        "    utility_matrix = np.zeros((3,4))\n",
        "    gamma = 0.999 #discount rate\n",
        "    alpha = 0.1 #constant step size\n",
        "    lambda_ = 0.5 #decaying factor\n",
        "    tot_epoch = 300000\n",
        "    print_epoch = 100\n",
        "\n",
        "    for epoch in range(tot_epoch):\n",
        "        #Reset and return the first observation\n",
        "        observation = env.reset(exploring_starts=True)\n",
        "        for step in range(1000):\n",
        "            #Take the action from the action matrix\n",
        "            action = policy_matrix[observation[0], observation[1]]\n",
        "            #Move one step in the environment and get obs and reward\n",
        "            new_observation, reward, done = env.step(action)\n",
        "            #Estimate the error delta (Target - OldEstimate)\n",
        "            delta = reward + gamma * utility_matrix[new_observation[0], new_observation[1]] - \\\n",
        "                                     utility_matrix[observation[0], observation[1]]\n",
        "            #Adding +1 in the trace matrix for the state visited\n",
        "            trace_matrix[observation[0], observation[1]] += 1\n",
        "            #Update the utility matrix\n",
        "            utility_matrix = update_utility(utility_matrix, trace_matrix, alpha, delta)\n",
        "            #Update the trace matrix (decaying)\n",
        "            trace_matrix = update_eligibility(trace_matrix, gamma, lambda_)\n",
        "            observation = new_observation\n",
        "            if done: break #return\n",
        "\n",
        "        if(epoch % print_epoch == 0):\n",
        "            print(\"\")\n",
        "            print(\"Utility matrix after \" + str(epoch+1) + \" iterations:\")\n",
        "            print(utility_matrix)\n",
        "            print(trace_matrix)\n",
        "    #Time to check the utility matrix obtained\n",
        "    print(\"Utility matrix after \" + str(tot_epoch) + \" iterations:\")\n",
        "    print(utility_matrix)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mO5g3deBjlsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARSA: Python and ε-greedy policy"
      ],
      "metadata": {
        "id": "6CBdrnrJyT55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### greedy_prob = 1 - epsilon + non_greedy_prob\n",
        "##### Most of the time a value of 0.1 is a good choice for epsilon. Choosing a value that is too high will cause the algorithm to converge slowly (too much exploration). On the other hand, a value which is too small does not guarantee to visit all the state-action pairs leading to sub-optimal policies."
      ],
      "metadata": {
        "id": "DGyTlf-Uq9ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### state_action_matrix 에서 best_action을 policy matrix에 업데이트 -> 새로운 스테이트-> 해당 스테이트에 해당하는 action을 policy_matrix에서 가져온다. -> 해당 action에 greedy p을 배정하고  다른 action에는 non-greedy p를 배정 -> 이렇게 배정확률기준으로 random하게 action을 선택-> 이제 스테이트와 action이 선택되었음으로 이를 지기준을 vaule를 계산하고 이를 state_action_matrix에 업데이트 ---"
      ],
      "metadata": {
        "id": "hnJycjdXsqlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_state_action(state_action_matrix, visit_counter_matrix, observation, new_observation,\n",
        "                   action, new_action, reward, alpha, gamma):\n",
        "    '''Return the updated utility matrix\n",
        "\n",
        "    @param state_action_matrix the matrix before the update\n",
        "    @param observation the state obsrved at t\n",
        "    @param new_observation the state observed at t+1\n",
        "    @param action the action at t\n",
        "    @param new_action the action at t+1\n",
        "    @param reward the reward observed after the action\n",
        "    @param alpha the ste size (learning rate)\n",
        "    @param gamma the discount factor\n",
        "    @return the updated state action matrix\n",
        "    '''\n",
        "    #Getting the values of Q at t and at t+1\n",
        "    col = observation[1] + (observation[0]*4)\n",
        "    q = state_action_matrix[action, col]\n",
        "    col_t1 = new_observation[1] + (new_observation[0]*4)\n",
        "    q_t1 = state_action_matrix[int(new_action) ,col_t1]\n",
        "    #Calculate alpha based on how many time it\n",
        "    #has been visited\n",
        "    alpha_counted = 1.0 / (1.0 + visit_counter_matrix[action, col])\n",
        "    #Applying the update rule\n",
        "    #Here you can change \"alpha\" with \"alpha_counted\" if you want\n",
        "    #to take into account how many times that particular state-action\n",
        "    #pair has been visited until now.\n",
        "    state_action_matrix[action ,col] = state_action_matrix[action ,col] + alpha * (reward + gamma * q_t1 - q)\n",
        "    return state_action_matrix\n",
        "\n",
        "def update_visit_counter(visit_counter_matrix, observation, action):\n",
        "    '''Update the visit counter\n",
        "\n",
        "    Counting how many times a state-action pair has been\n",
        "    visited. This information can be used during the update.\n",
        "    @param visit_counter_matrix a matrix initialised with zeros\n",
        "    @param observation the state observed\n",
        "    @param action the action taken\n",
        "    '''\n",
        "    col = observation[1] + (observation[0]*4)\n",
        "    visit_counter_matrix[action ,col] += 1.0\n",
        "    return visit_counter_matrix\n",
        "\n",
        "def update_policy(policy_matrix, state_action_matrix, observation):\n",
        "    '''Return the updated policy matrix\n",
        "\n",
        "    @param policy_matrix the matrix before the update\n",
        "    @param state_action_matrix the state-action matrix\n",
        "    @param observation the state obsrved at t\n",
        "    @return the updated state action matrix\n",
        "    '''\n",
        "    col = observation[1] + (observation[0]*4)\n",
        "    #Getting the index of the action with the highest utility\n",
        "    best_action = np.argmax(state_action_matrix[:, col])\n",
        "    #Updating the policy\n",
        "    policy_matrix[observation[0], observation[1]] = best_action\n",
        "    return policy_matrix\n",
        "\n",
        "def return_epsilon_greedy_action(policy_matrix, observation, epsilon=0.1):\n",
        "    '''Return an action choosing it with epsilon-greedy\n",
        "\n",
        "    @param policy_matrix the matrix before the update\n",
        "    @param observation the state obsrved at t\n",
        "    @param epsilon the value used for computing the probabilities\n",
        "    @return the updated policy_matrix\n",
        "    '''\n",
        "    tot_actions = int(np.nanmax(policy_matrix) + 1)\n",
        "    # policy_matrix에는 best action만 업데이트 따라서 best_action에 가장높은 확률을 나머지 action에는 기본확률 배정\n",
        "    action = int(policy_matrix[observation[0], observation[1]])\n",
        "    non_greedy_prob = epsilon / tot_actions\n",
        "    greedy_prob = 1 - epsilon + non_greedy_prob\n",
        "    weight_array = np.full((tot_actions), non_greedy_prob)\n",
        "    weight_array[action] = greedy_prob\n",
        "    return np.random.choice(tot_actions, 1, p=weight_array)\n",
        "\n",
        "def print_policy(policy_matrix):\n",
        "    '''Print the policy using specific symbol.\n",
        "\n",
        "    * terminal state\n",
        "    ^ > v < up, right, down, left\n",
        "    # obstacle\n",
        "    '''\n",
        "    counter = 0\n",
        "    shape = policy_matrix.shape\n",
        "    policy_string = \"\"\n",
        "    for row in range(shape[0]):\n",
        "        for col in range(shape[1]):\n",
        "            if(policy_matrix[row,col] == -1): policy_string += \" *  \"\n",
        "            elif(policy_matrix[row,col] == 0): policy_string += \" ^  \"\n",
        "            elif(policy_matrix[row,col] == 1): policy_string += \" >  \"\n",
        "            elif(policy_matrix[row,col] == 2): policy_string += \" v  \"\n",
        "            elif(policy_matrix[row,col] == 3): policy_string += \" <  \"\n",
        "            elif(np.isnan(policy_matrix[row,col])): policy_string += \" #  \"\n",
        "            counter += 1\n",
        "        policy_string += '\\n'\n",
        "    print(policy_string)\n",
        "\n",
        "def return_decayed_value(starting_value, global_step, decay_step):\n",
        "        \"\"\"Returns the decayed value.\n",
        "\n",
        "        decayed_value = starting_value * decay_rate ^ (global_step / decay_steps)\n",
        "        @param starting_value the value before decaying\n",
        "        @param global_step the global step to use for decay (positive integer)\n",
        "        @param decay_step the step at which the value is decayed\n",
        "        \"\"\"\n",
        "        decayed_value = starting_value * np.power(0.1, (global_step/decay_step))\n",
        "        return decayed_value"
      ],
      "metadata": {
        "id": "f-wNsvvhk5SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    env = GridWorld(3, 4)\n",
        "\n",
        "    #Define the state matrix\n",
        "    state_matrix = np.zeros((3,4))\n",
        "    state_matrix[0, 3] = 1\n",
        "    state_matrix[1, 3] = 1\n",
        "    state_matrix[1, 1] = -1\n",
        "    print(\"State Matrix:\")\n",
        "    print(state_matrix)\n",
        "\n",
        "    #Define the reward matrix\n",
        "    reward_matrix = np.full((3,4), -0.04)\n",
        "    reward_matrix[0, 3] = 1\n",
        "    reward_matrix[1, 3] = -1\n",
        "    print(\"Reward Matrix:\")\n",
        "    print(reward_matrix)\n",
        "\n",
        "    #Define the transition matrix\n",
        "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
        "                                  [0.1, 0.8, 0.1, 0.0],\n",
        "                                  [0.0, 0.1, 0.8, 0.1],\n",
        "                                  [0.1, 0.0, 0.1, 0.8]])\n",
        "\n",
        "    #Random policy\n",
        "    policy_matrix = np.random.randint(low=0, high=4, size=(3, 4)).astype(np.float32)\n",
        "    policy_matrix[1,1] = np.nan #NaN for the obstacle at (1,1)\n",
        "    policy_matrix[0,3] = policy_matrix[1,3] = -1 #No action for the terminal states\n",
        "    print(\"Policy Matrix:\")\n",
        "    print(policy_matrix)\n",
        "\n",
        "    env.setStateMatrix(state_matrix)\n",
        "    env.setRewardMatrix(reward_matrix)\n",
        "    env.setTransitionMatrix(transition_matrix)\n",
        "\n",
        "    #utility_matrix = np.zeros((3,4))\n",
        "    state_action_matrix = np.zeros((4,12))\n",
        "    visit_counter_matrix = np.zeros((4,12))\n",
        "    gamma = 0.999\n",
        "    alpha = 0.001 #constant step size\n",
        "    tot_epoch = 5000000\n",
        "    print_epoch = 1000\n",
        "\n",
        "\n",
        "    for epoch in range(tot_epoch):\n",
        "\n",
        "        # epoch이 증가할 수록 greedy 확률이 더 커지도록 설정 --> 학습이 진행될수록\n",
        "        # best_action이 다시 선택될 확률을 높임으로써 수렴을 빠르게 한다. --> local 빠지게 하는 결과를 가져올 수도 있다.\n",
        "        epsilon = return_decayed_value(0.1, epoch, decay_step=100000)\n",
        "        #Reset and return the first observation\n",
        "        observation = env.reset(exploring_starts=True)\n",
        "        is_starting = True\n",
        "        for step in range(1000):\n",
        "            #Take the action from the action matrix\n",
        "            #action = policy_matrix[observation[0], observation[1]]\n",
        "            #Take the action using epsilon-greedy\n",
        "            action = return_epsilon_greedy_action(policy_matrix, observation, epsilon=0.1)\n",
        "            if(is_starting):\n",
        "                action = np.random.randint(0, 4)\n",
        "                is_starting = False\n",
        "            #Move one step in the environment and get obs and reward\n",
        "            new_observation, reward, done = env.step(action)\n",
        "            new_action = policy_matrix[new_observation[0], new_observation[1]]\n",
        "            #Updating the state-action matrix\n",
        "            state_action_matrix = update_state_action(state_action_matrix, visit_counter_matrix, observation, new_observation,\n",
        "                                                      action, new_action, reward, alpha, gamma)\n",
        "            #Updating the policy\n",
        "            policy_matrix = update_policy(policy_matrix, state_action_matrix, observation)\n",
        "            #Increment the visit counter\n",
        "            visit_counter_matrix = update_visit_counter(visit_counter_matrix, observation, action)\n",
        "            observation = new_observation\n",
        "            #print(utility_matrix)\n",
        "            if done: break\n",
        "\n",
        "        if(epoch % print_epoch == 0):\n",
        "            print(\"\")\n",
        "            print(\"Epsilon: \" + str(epsilon))\n",
        "            print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\")\n",
        "            print(state_action_matrix)\n",
        "            print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\")\n",
        "            print_policy(policy_matrix)\n",
        "\n",
        "    #Time to check the utility matrix obtained\n",
        "    print(\"State-Action matrix after \" + str(tot_epoch) + \" iterations:\")\n",
        "    print(state_action_matrix)\n",
        "    print(\"Policy matrix after \" + str(tot_epoch) + \" iterations:\")\n",
        "    print_policy(policy_matrix)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "8HA0DwXc1jZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-learning: off-policy control"
      ],
      "metadata": {
        "id": "17TspaEQ1X0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def update_state_action(state_action_matrix, visit_counter_matrix, observation, new_observation,\n",
        "                        action, reward, alpha, gamma):\n",
        "    '''Return the updated utility matrix\n",
        "\n",
        "    @param state_action_matrix the matrix before the update\n",
        "    @param observation the state obsrved at t\n",
        "    @param new_observation the state observed at t+1\n",
        "    @param action the action at t\n",
        "    @param new_action the action at t+1\n",
        "    @param reward the reward observed after the action\n",
        "    @param alpha the ste size (learning rate)\n",
        "    @param gamma the discount factor\n",
        "    @return the updated state action matrix\n",
        "    '''\n",
        "    #Getting the values of Q at t and at t+1\n",
        "    col = observation[1] + (observation[0]*4)\n",
        "    q = state_action_matrix[action ,col]\n",
        "    col_t1 = new_observation[1] + (new_observation[0]*4)\n",
        "    q_t1 = np.max(state_action_matrix[: ,col_t1])\n",
        "    #Calculate alpha based on how many time it\n",
        "    #has been visited\n",
        "    alpha_counted = 1.0 / (1.0 + visit_counter_matrix[action, col])\n",
        "    #Applying the update rule\n",
        "    #Here you can change \"alpha\" with \"alpha_counted\" if you want\n",
        "    #to take into account how many times that particular state-action\n",
        "    #pair has been visited until now.\n",
        "    state_action_matrix[action ,col] = state_action_matrix[action ,col] + alpha * (reward + gamma * q_t1 - q)\n",
        "    return state_action_matrix\n",
        "\n",
        "def update_visit_counter(visit_counter_matrix, observation, action):\n",
        "    '''Update the visit counter\n",
        "\n",
        "    Counting how many times a state-action pair has been\n",
        "    visited. This information can be used during the update.\n",
        "    @param visit_counter_matrix a matrix initialised with zeros\n",
        "    @param observation the state observed\n",
        "    @param action the action taken\n",
        "    '''\n",
        "    col = observation[1] + (observation[0]*4)\n",
        "    visit_counter_matrix[action ,col] += 1.0\n",
        "    return visit_counter_matrix\n",
        "\n",
        "def update_policy(policy_matrix, state_action_matrix, observation):\n",
        "    '''Return the updated policy matrix (q-learning)\n",
        "\n",
        "    @param policy_matrix the matrix before the update\n",
        "    @param state_action_matrix the state-action matrix\n",
        "    @param observation the state obsrved at t\n",
        "    @return the updated state action matrix\n",
        "    '''\n",
        "    col = observation[1] + (observation[0]*4)\n",
        "    #Getting the index of the action with the highest utility\n",
        "    best_action = np.argmax(state_action_matrix[:, col])\n",
        "    #Updating the policy\n",
        "    policy_matrix[observation[0], observation[1]] = best_action\n",
        "    return policy_matrix\n",
        "\n",
        "def return_epsilon_greedy_action(policy_matrix, observation, epsilon=0.1):\n",
        "    tot_actions = int(np.nanmax(policy_matrix) + 1)\n",
        "    action = int(policy_matrix[observation[0], observation[1]])\n",
        "    non_greedy_prob = epsilon / tot_actions\n",
        "    greedy_prob = 1 - epsilon + non_greedy_prob\n",
        "    weight_array = np.full((tot_actions), non_greedy_prob)\n",
        "    weight_array[action] = greedy_prob\n",
        "    return np.random.choice(tot_actions, 1, p=weight_array)\n",
        "\n",
        "def print_policy(policy_matrix):\n",
        "    '''Print the policy using specific symbol.\n",
        "\n",
        "    * terminal state\n",
        "    ^ > v < up, right, down, left\n",
        "    # obstacle\n",
        "    '''\n",
        "    counter = 0\n",
        "    shape = policy_matrix.shape\n",
        "    policy_string = \"\"\n",
        "    for row in range(shape[0]):\n",
        "        for col in range(shape[1]):\n",
        "            if(policy_matrix[row,col] == -1): policy_string += \" *  \"\n",
        "            elif(policy_matrix[row,col] == 0): policy_string += \" ^  \"\n",
        "            elif(policy_matrix[row,col] == 1): policy_string += \" >  \"\n",
        "            elif(policy_matrix[row,col] == 2): policy_string += \" v  \"\n",
        "            elif(policy_matrix[row,col] == 3): policy_string += \" <  \"\n",
        "            elif(np.isnan(policy_matrix[row,col])): policy_string += \" #  \"\n",
        "            counter += 1\n",
        "        policy_string += '\\n'\n",
        "    print(policy_string)\n",
        "\n",
        "def return_decayed_value(starting_value, global_step, decay_step):\n",
        "        \"\"\"Returns the decayed value.\n",
        "\n",
        "        decayed_value = starting_value * decay_rate ^ (global_step / decay_steps)\n",
        "        @param starting_value the value before decaying\n",
        "        @param global_step the global step to use for decay (positive integer)\n",
        "        @param decay_step the step at which the value is decayed\n",
        "        \"\"\"\n",
        "        decayed_value = starting_value * np.power(0.1, (global_step/decay_step))\n",
        "        return decayed_value\n"
      ],
      "metadata": {
        "id": "XnlwKjlCsCjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    env = GridWorld(3, 4)\n",
        "\n",
        "    #Define the state matrix\n",
        "    state_matrix = np.zeros((3,4))\n",
        "    state_matrix[0, 3] = 1\n",
        "    state_matrix[1, 3] = 1\n",
        "    state_matrix[1, 1] = -1\n",
        "    print(\"State Matrix:\")\n",
        "    print(state_matrix)\n",
        "\n",
        "    #Define the reward matrix\n",
        "    reward_matrix = np.full((3,4), -0.04)\n",
        "    reward_matrix[0, 3] = 1\n",
        "    reward_matrix[1, 3] = -1\n",
        "    print(\"Reward Matrix:\")\n",
        "    print(reward_matrix)\n",
        "\n",
        "    #Define the transition matrix\n",
        "    transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
        "                                  [0.1, 0.8, 0.1, 0.0],\n",
        "                                  [0.0, 0.1, 0.8, 0.1],\n",
        "                                  [0.1, 0.0, 0.1, 0.8]])\n",
        "\n",
        "    #Random policy\n",
        "    policy_matrix = np.random.randint(low=0, high=4, size=(3, 4)).astype(np.float32)\n",
        "    policy_matrix[1,1] = np.nan #NaN for the obstacle at (1,1)\n",
        "    policy_matrix[0,3] = policy_matrix[1,3] = -1 #No action for the terminal states\n",
        "    print(\"Policy Matrix:\")\n",
        "    print(policy_matrix)\n",
        "    print_policy(policy_matrix)\n",
        "\n",
        "    #Adversarial exploration policy\n",
        "    #exploratory_policy_matrix = np.array([[2,      3, 2, -1],\n",
        "                                          #[2, np.nan, 1, -1],\n",
        "                                          #[1,      1, 1,  0]])\n",
        "\n",
        "    exploratory_policy_matrix = np.array([[1,      1, 1, -1],\n",
        "                                          [0, np.nan, 0, -1],\n",
        "                                          [0,      1, 0,  3]])\n",
        "\n",
        "    print(\"Exploratory Policy Matrix:\")\n",
        "    print(exploratory_policy_matrix)\n",
        "    print_policy(exploratory_policy_matrix)\n",
        "\n",
        "    env.setStateMatrix(state_matrix)\n",
        "    env.setRewardMatrix(reward_matrix)\n",
        "    env.setTransitionMatrix(transition_matrix)\n",
        "\n",
        "    state_action_matrix = np.zeros((4,12))\n",
        "    visit_counter_matrix = np.zeros((4,12))\n",
        "    gamma = 0.999\n",
        "    alpha = 0.001 #constant step size\n",
        "    tot_epoch = 5000000\n",
        "    print_epoch = 1000\n",
        "\n",
        "    for epoch in range(tot_epoch):\n",
        "        #Reset and return the first observation\n",
        "        observation = env.reset(exploring_starts=True)\n",
        "        epsilon = return_decayed_value(0.1, epoch, decay_step=50000)\n",
        "        is_starting = True\n",
        "        for step in range(1000):\n",
        "            #Take the action from the action matrix\n",
        "            #action = policy_matrix[observation[0], observation[1]]\n",
        "            #Take the action using epsilon-greedy\n",
        "            action = return_epsilon_greedy_action(exploratory_policy_matrix, observation, epsilon=0.001)\n",
        "            if(is_starting):\n",
        "                action = np.random.randint(0, 4)\n",
        "                is_starting = False\n",
        "            #Move one step in the environment and get obs and reward\n",
        "            new_observation, reward, done = env.step(action)\n",
        "            #Updating the state-action matrix\n",
        "            state_action_matrix = update_state_action(state_action_matrix, visit_counter_matrix, observation, new_observation,\n",
        "                                                      action, reward, alpha, gamma)\n",
        "            #Updating the policy\n",
        "            policy_matrix = update_policy(policy_matrix, state_action_matrix, observation)\n",
        "            #Increment the visit counter\n",
        "            visit_counter_matrix = update_visit_counter(visit_counter_matrix, observation, action)\n",
        "            observation = new_observation\n",
        "            if done: break\n",
        "\n",
        "        if(epoch % print_epoch == 0):\n",
        "            print(\"\")\n",
        "            print(\"Epsilon: \" + str(epsilon))\n",
        "            print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\")\n",
        "            print(state_action_matrix)\n",
        "            print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\")\n",
        "            print_policy(policy_matrix)\n",
        "\n",
        "    #Time to check the utility matrix obtained\n",
        "    print(\"State-Action matrix after \" + str(tot_epoch) + \" iterations:\")\n",
        "    print(state_action_matrix)\n",
        "    print(\"Policy matrix after \" + str(tot_epoch) + \" iterations:\")\n",
        "    print_policy(policy_matrix)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "qBCRSIo91pr_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}